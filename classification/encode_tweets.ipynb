{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import cosine sim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import mlp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "# import pca\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from run_all import fit_optimal_transport, fit_steering_intervention\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/shauli/miniconda3/envs/py39 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:38<00:00, 79.08s/it] \n"
     ]
    }
   ],
   "source": [
    "#model_name = \"gpt2\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "load_in_8bit=True if \"llama\" in model_name else False\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=load_in_8bit,trust_remote_code=True,torch_dtype=torch.float16,\n",
    "                                             device_map='auto').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(model, tokenizer, text, batch_size, ln_module = None,layer=-1):\n",
    "  encodings_last, encodings_mean = [], []\n",
    "  logits_last = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "   for i in tqdm.tqdm(range(0, len(text), batch_size)):\n",
    "    batch = text[i:i+batch_size]\n",
    "    padded_tokens = tokenizer(batch, padding=True, return_tensors=\"pt\", max_length=128, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**padded_tokens, output_hidden_states=True)\n",
    "      lengths = padded_tokens[\"attention_mask\"].sum(axis=1).detach().cpu().numpy()\n",
    "\n",
    "    hiddens = outputs.hidden_states[layer]#.detach().cpu().numpy()\n",
    "    logits_all = outputs.logits\n",
    "    hiddens = hiddens.detach()\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    for h,logits,l in zip(hiddens, logits_all, lengths):\n",
    "      h_last = h[l-1]\n",
    "      l_last = logits[l-1]\n",
    "      if ln_module is not None:\n",
    "        h_last = ln_module(h_last)\n",
    "      encodings_last.append(h_last.detach().cpu().numpy().tolist())\n",
    "\n",
    "  logits_last = np.array(logits_last)\n",
    "  return np.array(encodings_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"tweets-data/dialect_sentiment_data/\"\n",
    "model_str = model_name.split(\"/\")[-1]\n",
    "for filename in \"neg_neg.txt\", \"neg_pos.txt\", \"pos_neg.txt\", \"pos_pos.txt\":\n",
    "  with open(base_path+filename, errors=\"replace\") as f:\n",
    "    text = [line.strip() for line in f.readlines()][:44000]\n",
    "    print(len(text))\n",
    "  encodings = encode(model, tokenizer, text, 64, layer=-1)\n",
    "  path_to_save = base_path + model_str + \"_\" + filename.split(\".\")[0] + \".npy\"\n",
    "  np.save(path_to_save, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bios_data/bios_data/bios_train.pickle\", \"rb\") as f:\n",
    "  bios_train = pickle.load(f)\n",
    "\n",
    "with open(\"bios_data/bios_data/bios_dev.pickle\", \"rb\") as f:\n",
    "  bios_dev = pickle.load(f)\n",
    "\n",
    "with open(\"bios_data/bios_data/bios_test.pickle\", \"rb\") as f:\n",
    "  bios_test = pickle.load(f)\n",
    "\n",
    "path = \"bios_data/bios_data/\"\n",
    "\n",
    "\n",
    "\n",
    "prefix = \"\"\n",
    "text_train = [prefix+d[\"hard_text\"] for d in bios_train]\n",
    "text_dev = [prefix+d[\"hard_text\"] for d in bios_dev]\n",
    "text_test = [prefix+d[\"hard_text\"] for d in bios_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1537 [00:08<55:32,  2.17s/it]"
     ]
    }
   ],
   "source": [
    "base_path = \"bios_data/bios_data/\"\n",
    "model_str = model_name.split(\"/\")[-1]\n",
    "texts = [text_test]\n",
    "splits = [\"test\"]\n",
    "\n",
    "for text, split in zip(texts, splits):\n",
    "  encodings = encode(model, tokenizer, text[:], 64, layer=-1)\n",
    "  path_to_save = base_path + model_str + \"_\" + split + \".npy\"\n",
    "  np.save(path_to_save, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bios_data/bios_data/Llama-2-7b-chat-hf_t.npy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+filename, \"rb\") as f:\n",
    "    f.read().decode(\"utf-8\", errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.load(\"tweets-data/neg_neg.npy\")\n",
    "x2 = np.load(\"tweets-data/neg_pos.npy\")\n",
    "x3 = np.load(\"tweets-data/pos_neg.npy\")\n",
    "x4 = np.load(\"tweets-data/pos_pos.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
